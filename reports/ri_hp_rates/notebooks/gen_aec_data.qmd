---
title: "Generate AEC Datasets"
---

Script to read ResStock data from S3 and generate datasets required by AEC


### IMPORTS
```{python}

    from buildstock_fetch.scenarios import uniform_adoption
    from buildstock_fetch.mixed_upgrade import MixedUpgradeScenario
    from buildstock_fetch.read import BuildStockRead

    import polars as pl
```

### INPUT PARAMS AND CONSTANTS
```{python}
    DEBUG_MODE = False

    data_path="s3://data.sb/nrel/resstock/"
    release="res_2024_amy2018_2"
    states="RI"
    random_seed=67
    sample_n=None # do not downsample

    MMBTU_PER_KWH = 0.003412  # 1 kWh = 0.003412 MMBtu

    projection_years = [2028, 2029, 2030, 2031]
    start_year = projection_years[0]
    if not all([x+1 == y for x, y in zip(projection_years[:-1], projection_years[1:])]):
        raise ValueError("Projection years must be consecutive")

    total_num_homes = 451381 # from slack with Juan-Pablo

    # For now, using the 1.0 rates from the MA report
    # https://www.switch.box/mahprates
    # In MA, the 1.0 rates take HPs from being bill-reducing for 44.7% of homes
    # to 64.1% of homes, an improvement of 43%, so we assume that in the HP rates case
    # here in RI, a heat pump rate will increase HP adoption by 43%. That is, a similar share
    # of homes for which HPs are bill-reducing will choose to adopt HPs in both the BAU and HP cases.
    heat_pump_adoption_factor = 1.43

```

### HELPER FUNCTIONS
```{python}
    def adjust_timestamp(lazy_df, start_year, handle_leap_years=True):
        """
        Adjusts the timestamp column in a lazy Polars DataFrame so that each 'year' value corresponds
        to the specified start_year plus its offset.

        For example, if start_year=2018:
            - rows with year==0: timestamps correspond to 2018
            - rows with year==1: timestamps are shifted to 2019
            and so on.

        Args:
            lazy_df: Polars LazyFrame with 'timestamp' and 'year' columns.
            start_year: The baseline year (integer, e.g., 2018)
            handle_leap_years: Whether to handle leap years by inserting a duplicate of Feb 28 data as Feb 29

        Returns:
            A new LazyFrame with the 'timestamp' column adjusted so that each 'year' is mapped
            to start_year + year, preserving month/day/hour/minute.
        """

        # Adjust the year of each timestamp so that it matches start_year + year
        updated_df= lazy_df.rename(
            {"timestamp": "original_timestamp",
            "year": "year_idx"}
            ).with_columns(
                pl.col("year_idx").add(start_year).alias("year")
            ).with_columns(
            pl.col("original_timestamp").dt.replace(year=pl.col("year")).alias("timestamp")
        )

        if handle_leap_years:
            # NOTE: Handle leap years by inserting a duplicate of Feb 28 data as Feb 29
            # We maybe exclude this to make data more generally interpretable across years?
            # Insert a duplicate of Feb 28 data as Feb 29 in leap years
            feb_28 = (
                updated_df
                .filter((pl.col("timestamp").dt.month() == 2) &
                        (pl.col("timestamp").dt.day() == 28) &
                        ((pl.col("year") % 4) == 0))
            )

            # Create Feb 29 timestamps by changing day = 29 on a copy of Feb 28 rows
            feb_29 = (
                feb_28
                .with_columns(
                    pl.col("timestamp").dt.replace(day=29).alias("timestamp"),
                )
            )

            # Concatenate the original updated_df with the new Feb 29 rows
            updated_df = pl.concat([updated_df, feb_29]).sort("timestamp")

        return updated_df
```

### AUTHORIZE GOOGLE SHEET
```{python}
    import os
    import gspread
    from dotenv import load_dotenv

    # Load variables from .env
    load_dotenv()

    # Reconstruct the credential dictionary
    app_creds = {
        "installed": {
            "client_id": os.getenv("G_CLIENT_ID"),
            "client_secret": os.getenv("G_CLIENT_SECRET"),
            "project_id": os.getenv("G_PROJECT_ID"),
            "auth_uri": os.getenv("G_AUTH_URI"),
            "token_uri": os.getenv("G_TOKEN_URI"),
            "redirect_uris": ["http://localhost"]
        }
    }

    # This will trigger the browser login for the user
    gc, authorized_user = gspread.oauth_from_dict(credentials=app_creds)
```


### ADOPTION RATE ESTIMATES
```{python}
    def get_adoption_forecast_num_homes(projection_years: list) -> list[float]:
        rie_proj_sheet = gc.open_by_url("https://docs.google.com/spreadsheets/d/1BX4rp256SB4vI72oUIbZT1BqD6gOO8UJr95578dDM-Q/    edit#gid=2027836829").worksheet("RIE_hp_adoption_forecast")
        # Get all data as list of lists
        rows = rie_proj_sheet.get_all_values()
        start_idx = None
        num_cols = None
        for i, row in enumerate(rows):
            if row[0] == "Year":
                start_idx = i
                for j, col in enumerate(row):
                    if col == "Cumulative_Low":
                        num_cols = j
                        break
                break

        df = pl.DataFrame([row[:num_cols] for row in rows[start_idx+1:]], schema=rows[start_idx][:num_cols], orient="row")

        return df.filter(pl.col("Year").is_in([str(x) for x in projection_years])).select(pl.col("Cumulative_Base")).cast(pl.Float64).to_series().to_list()
```

```{python}
    sbmeta = pl.scan_parquet("s3://data.sb/nrel/resstock/res_2024_amy2018_2/metadata/state=RI/upgrade=00/metadata-sb-with-heating-type.parquet")

    current_adoption_rate = sbmeta.collect().group_by("postprocess_group.heating_type").agg(pl.col("weight").sum().alias("total_homes")).pipe(lambda df: df.filter(pl.col("postprocess_group.heating_type") == "heat_pump")["total_homes"] / df["total_homes"].sum()).item()

    if DEBUG_MODE:
        print(current_adoption_rate) # NOTE: DEBUGGING ONLY

    # Get adoption forecast number of homes
    bau_hp_counts = get_adoption_forecast_num_homes([2028, 2029, 2030, 2031])
    bau_pct_projections = [x / total_num_homes for x in bau_hp_counts]

    bau_pct_total = [x / total_num_homes for x in bau_hp_counts]
    hp_pct_total = [heat_pump_adoption_factor * x / total_num_homes for x in bau_hp_counts]
    bau_pct_upgrade_1 = [(x - current_adoption_rate) / (1 - current_adoption_rate) for x in bau_pct_total]
    hp_pct_upgrade_1 = [(x - current_adoption_rate) / (1 - current_adoption_rate) for x in hp_pct_total]
```

### GET LOAD CURVES FROM RESSTOCK USING BSF
```{python}
    bau_scenario = uniform_adoption(
        upgrade_ids=[1],
        weights={1: 1.0},
        adoption_trajectory=bau_pct_upgrade_1, # see above
    )

    hp_scenario = uniform_adoption(
        upgrade_ids=[1],
        weights={1: 1.0},
        adoption_trajectory=hp_pct_upgrade_1, # see above
    )

    def _merge_weights(ldf, metadata = pl.DataFrame()):
        return ldf.join(metadata.select(["bldg_id", "weight"]), on="bldg_id", how="left")

    # NOTE: add typehint
    def _get_load_curves(scenario, scenario_name):
        mus = MixedUpgradeScenario(
            scenario_name=scenario_name,
            data_path=data_path,
            release=release,
            states=states,
            # NOTE:
            # this is maybe weird, in that the set
            # of adopting households in BAU isn't a subset of the set
            # of adopting households in HP
            random=random_seed,
            sample_n=sample_n,
            scenario=scenario,
        )
        # NOTE: should this be piped for something something efficiency
        ldf = mus.read_load_curve_hourly().pipe(
            lambda ldf: adjust_timestamp(ldf, start_year=start_year)
        ).pipe(lambda ldf: _merge_weights(ldf, metadata=sbmeta))
        return mus, ldf

    # Read 15-minute load curves as lazy dataframe
    bau_mus, ldf_bau_loads = _get_load_curves(bau_scenario, "bau")
    hp_mus, ldf_hp_loads = _get_load_curves(hp_scenario, "hp")
```

```{python}
def sum_energy_by_timestamp(lazy_df):
    """
    For each timestamp, sums up various energy consumption values across all bldg_id.

    Args:
        lazy_df: A polars lazy dataframe

    Returns:
        A polars dataframe (not lazy) with summed columns per timestamp.
    """
    selected_cols = [
        "timestamp",
        "out.electricity.net.energy_consumption",
        "out.fuel_oil.total.energy_consumption",
        "out.natural_gas.total.energy_consumption",
        "out.propane.total.energy_consumption",
        "weight",
    ]
    return (
        lazy_df
        .select(selected_cols)
        .group_by("timestamp")
        .agg([
            (pl.col("weight") * pl.col("out.electricity.net.energy_consumption")).sum().alias("electricity_kwh"),
            (MMBTU_PER_KWH * pl.col("weight") * pl.col("out.fuel_oil.total.energy_consumption")).sum().alias("fuel_oil_mmbtu"),
            (MMBTU_PER_KWH * pl.col("weight") * pl.col("out.natural_gas.total.energy_consumption")).sum().alias("natural_gas_mmbtu"),
            (MMBTU_PER_KWH * pl.col("weight") * pl.col("out.propane.total.energy_consumption")).sum().alias("propane_mmbtu"),
        ])
        .sort("timestamp")
        .collect()
    )
```

```{python}
    %%time
    df_bau_loads = sum_energy_by_timestamp(ldf_bau_loads)
    df_hp_loads = sum_energy_by_timestamp(ldf_hp_loads)

    # print(len(df_bau_loads))
    # print(len(df_hp_loads))
    # print(df_bau_loads.head())
    # print(df_hp_loads.head())
```

```{python}
    # NOTE: use for saving load files to disk to avoid rerunning the long sum_energy_by_timestamp step
    if DEBUG_MODE:
        df_bau_loads.write_parquet("../../ri_hp_rates/notebooks/nocommit_bau_loads.parquet")
        df_hp_loads.write_parquet("../../ri_hp_rates/notebooks/nocommit_hp_loads.parquet")
```

```{python}

if DEBUG_MODE:
    from plotnine import ggplot, aes, geom_line, labs, theme_minimal, scale_color_manual

    # Prepare data for plotting; add labels for scenario
    df_bau_loads_plot = df_bau_loads.with_columns([
        pl.lit("BAU").alias("scenario")
    ])
    df_hp_loads_plot = df_hp_loads.with_columns([
        pl.lit("Heat Pump").alias("scenario")
    ])

    # Concatenate into one DataFrame
    plot_df = df_bau_loads_plot.vstack(df_hp_loads_plot)

    # plotnine expects a pandas DataFrame, so as a temporary, minimal conversion for plotting:
    # Filter to just February 2028
    feb_2028_df = plot_df.filter(
        (pl.col("timestamp").dt.year() == 2028) & (pl.col("timestamp").dt.month() == 2)
        & (pl.col("timestamp").dt.day() >= 24)
    )

    p = (
        ggplot(feb_2028_df.to_pandas(), aes(x='timestamp', y='electricity_kwh', color='scenario'))
        + geom_line()
        + labs(
            title='Electricity Consumption: February 2028',
            x='Timestamp',
            y='Electricity Consumption (kWh)',
            color='Scenario'
        )
        + scale_color_manual(values=["#1f77b4", "#ff7f0e"])
        + theme_minimal()
    )

    p.show()
```

### WRITE FINAL OUTPUT TO GSHEET
```{python}
    def write_to_gsheet(df, sheet_name, worksheet_name):
        data = [df.columns] + df.to_numpy().tolist()
        sh = gc.open(sheet_name)
        worksheets = sh.worksheets()

        # TODO: make this more general for sheet sorting
        ref_sheet_idx = next((i for i, ws in enumerate(worksheets) if ws.title == "Relevant metrics"), None)

        if worksheet_name not in [worksheet.title for worksheet in worksheets]:
            sh.add_worksheet(worksheet_name, rows=df.height + 1, cols=df.width, index=ref_sheet_idx+1)
        else:
            worksheet = sh.worksheet(worksheet_name)
            worksheet.clear()
        worksheet = sh.worksheet(worksheet_name)
        worksheet.update(data, "A1", value_input_option="USER_ENTERED")
```

```{python}
    go_live = "YES_GO_LIVE"
```

```{python}
    if go_live == "YES_GO_LIVE":
        if DEBUG_MODE:
            raise ValueError("DEBUG_MODE is True, are you sure you want to go live?")
        target_sheet_name = "Switchbox BCA overview"
        go_live = "nope"
    else:
        target_sheet_name = "test_data_sheet"

    # NOTE: remove the "reverse" once sheet sorting is fixed in write_to_gsheet
    for col in reversed(df_bau_loads.columns[1:]):
        merged_df = df_bau_loads[["timestamp", col]].rename({col: f"bau_{col}"}).join(df_hp_loads[["timestamp",    col]].rename({col: f"hp_{col}"}), on="timestamp").with_columns([
            (pl.col(f"hp_{col}") - pl.col(f"bau_{col}")).alias(f"delta_{col}"),
            pl.col("timestamp").dt.strftime("%Y-%m-%d").cast(pl.Utf8).alias("date"),
            pl.col("timestamp").dt.strftime("%H:%M:%S").cast(pl.Utf8).alias("time"),
            ]).select(["date", "time", f"delta_{col}", f"bau_{col}", f"hp_{col}"])
        print(merged_df.head())
        write_to_gsheet(merged_df, target_sheet_name, f"1_{col}")
```
