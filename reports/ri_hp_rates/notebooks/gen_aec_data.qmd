---
title: "Generate AEC Datasets"
---

Script to read ResStock data from S3 and generate datasets required by AEC
```{python}

    from buildstock_fetch.scenarios import uniform_adoption
    from buildstock_fetch.mixed_upgrade import MixedUpgradeScenario
    from buildstock_fetch.read import BuildStockRead

    import polars as pl
```

```{python}
    def adjust_timestamp(lazy_df, start_year, handle_leap_years=True):
        """
        Adjusts the timestamp column in a lazy Polars DataFrame so that each 'year' value corresponds
        to the specified start_year plus its offset.

        For example, if start_year=2018:
            - rows with year==0: timestamps correspond to 2018
            - rows with year==1: timestamps are shifted to 2019
            and so on.

        Args:
            lazy_df: Polars LazyFrame with 'timestamp' and 'year' columns.
            start_year: The baseline year (integer, e.g., 2018)

        Returns:
            A new LazyFrame with the 'timestamp' column adjusted so that each 'year' is mapped
            to start_year + year, preserving month/day/hour/minute.
        """

        # Adjust the year of each timestamp so that it matches start_year + year
        updated_df= lazy_df.rename(
            {"timestamp": "original_timestamp",
            "year": "year_idx"}
            ).with_columns(
                pl.col("year_idx").add(start_year).alias("year")
            ).with_columns(
            pl.col("original_timestamp").dt.replace(year=pl.col("year")).alias("timestamp")
        )

        if handle_leap_years:
            # NOTE: Handle leap years by inserting a duplicate of Feb 28 data as Feb 29
            # We maybe exclude this to make data more generally interpretable across years?
            # Insert a duplicate of Feb 28 data as Feb 29 in leap years
            feb_28 = (
                updated_df
                .filter((pl.col("timestamp").dt.month() == 2) &
                        (pl.col("timestamp").dt.day() == 28) &
                        ((pl.col("year") % 4) == 0))
            )

            # Create Feb 29 timestamps by changing day = 29 on a copy of Feb 28 rows
            feb_29 = (
                feb_28
                .with_columns(
                    pl.col("timestamp").dt.replace(day=29).alias("timestamp"),
                )
            )

            # Concatenate the original updated_df with the new Feb 29 rows
            updated_df = pl.concat([updated_df, feb_29]).sort("timestamp")

        return updated_df
```

```{python}
    total_num_homes = 451381 # from slack with Juan-Pablo
    projection_start_year = 2028
    # https://portalconnect.rienergy.com/RI/servlet/servlet.FileDownload?file=015Nu000006oDrR
    bau_hp_counts = [26221, 31666,38199, 44407]


    # TODO: hardcode this as well? or generally push off the s3 access to later in the code?
    sbmeta = pl.read_parquet("s3://data.sb/nrel/resstock/res_2024_amy2018_2/metadata/state=RI/upgrade=00/metadata-sb.parquet")
    current_adoption_rate = sbmeta.get_column("postprocess_group.has_hp").value_counts().filter(pl.col("postprocess_group.has_hp") == True).get_column("count")[0] / sbmeta.height
    print(current_adoption_rate)

    bau_pct_projections = [x / total_num_homes for x in bau_hp_counts]
    # TODO: come up with better hp rate adoption projections
    hp_pct_projections = [1.5 * x / total_num_homes for x in bau_hp_counts]

    # TODO: adjust the above based on current adoption rate in upgrade 0
    # to get correct final adoption rate in MixedUpgradeScenario
    bau_pct_upgrade_1 = [(x - current_adoption_rate) / (1 - current_adoption_rate) for x in bau_pct_projections]
    hp_pct_upgrade_1 = [(x - current_adoption_rate) / (1 - current_adoption_rate) for x in hp_pct_projections]

    print(bau_pct_projections)
    print(bau_pct_upgrade_1)
    print(hp_pct_projections)
    print(hp_pct_upgrade_1)
```

```{python}
    data_path="s3://data.sb/nrel/resstock/"
    release="res_2024_amy2018_2"
    states="RI"
    random_seed=67
    sample_n=None # do not downsample

    # Convert all *_kwh columns to *_mmbtu and drop the *_kwh columns
    MMBTU_PER_KWH = 0.003412  # 1 kWh = 0.003412 MMBtu

    START_YEAR = 2028

    bau_scenario = uniform_adoption(
        upgrade_ids=[1],
        weights={1: 1.0},
        adoption_trajectory=bau_pct_upgrade_1, # see above
    )

    hp_scenario = uniform_adoption(
        upgrade_ids=[1],
        weights={1: 1.0},
        adoption_trajectory=hp_pct_upgrade_1, # see above
    )

    # NOTE: add typehint
    def _get_load_curves(scenario, scenario_name):
        mus = MixedUpgradeScenario(
            scenario_name=scenario_name,
            data_path=data_path,
            release=release,
            states=states,
            # NOTE:
            # this is maybe weird, in that the set
            # of adopting households in BAU isn't a subset of the set
            # of adopting households in HP
            random=random_seed,
            sample_n=sample_n,
            scenario=scenario,
        )
        # NOTE: should this be piped for something something efficiency
        return mus, adjust_timestamp(mus.read_load_curve_hourly(), START_YEAR)

    # Read 15-minute load curves as lazy dataframe
    bau_mus, ldf_bau_loads = _get_load_curves(bau_scenario, "bau")
    hp_mus, ldf_hp_loads = _get_load_curves(hp_scenario, "hp")
```

```{python}
def sum_energy_by_timestamp(lazy_df):
    """
    For each timestamp, sums up various energy consumption values across all bldg_id.

    Args:
        lazy_df: A polars lazy dataframe

    Returns:
        A polars dataframe (not lazy) with summed columns per timestamp.
    """
    selected_cols = [
        "timestamp",
        "out.electricity.net.energy_consumption",
        "out.fuel_oil.total.energy_consumption",
        "out.natural_gas.total.energy_consumption",
        "out.propane.total.energy_consumption",
    ]
    return (
        lazy_df
        .select(selected_cols)
        .group_by("timestamp")
        .agg([
            pl.col("out.electricity.net.energy_consumption").sum().alias("electricity_kwh"),
            pl.col("out.fuel_oil.total.energy_consumption").sum().alias("fuel_oil_kwh"),
            pl.col("out.natural_gas.total.energy_consumption").sum().alias("natural_gas_kwh"),
            pl.col("out.propane.total.energy_consumption").sum().alias("propane_kwh"),
        ])
        .sort("timestamp")

        .with_columns([
            (pl.col("fuel_oil_kwh") * MMBTU_PER_KWH).alias("fuel_oil_mmbtu"),
            (pl.col("natural_gas_kwh") * MMBTU_PER_KWH).alias("natural_gas_mmbtu"),
            (pl.col("propane_kwh") * MMBTU_PER_KWH).alias("propane_mmbtu"),
        ])
        .drop(["fuel_oil_kwh", "natural_gas_kwh", "propane_kwh"])
        .collect()
    )
```

```{python}
    %%time
    df_bau_loads = sum_energy_by_timestamp(ldf_bau_loads)
    df_hp_loads = sum_energy_by_timestamp(ldf_hp_loads)

    print(len(df_bau_loads))
    print(len(df_hp_loads))
    print(df_bau_loads.head())
    print(df_hp_loads.head())
```

```{python}
    df_bau_loads.write_parquet("../../ri_hp_rates/notebooks/nocommit_data/bau_loads.parquet")
    df_hp_loads.write_parquet("../../ri_hp_rates/notebooks/nocommit_data/hp_loads.parquet")
```

```{python}
# NOTE: Debugging only, sample data plot

from plotnine import ggplot, aes, geom_line, labs, theme_minimal, scale_color_manual

# Prepare data for plotting; add labels for scenario
df_bau_loads_plot = df_bau_loads.with_columns([
    pl.lit("BAU").alias("scenario")
])
df_hp_loads_plot = df_hp_loads.with_columns([
    pl.lit("Heat Pump").alias("scenario")
])

# Concatenate into one DataFrame
plot_df = df_bau_loads_plot.vstack(df_hp_loads_plot)

# plotnine expects a pandas DataFrame, so as a temporary, minimal conversion for plotting:
# Filter to just February 2028
feb_2028_df = plot_df.filter(
    (pl.col("timestamp").dt.year() == 2028) & (pl.col("timestamp").dt.month() == 2)
    & (pl.col("timestamp").dt.day() >= 24)
)

p = (
    ggplot(feb_2028_df.to_pandas(), aes(x='timestamp', y='electricity_kwh', color='scenario'))
    + geom_line()
    + labs(
        title='Electricity Consumption: February 2028',
        x='Timestamp',
        y='Electricity Consumption (kWh)',
        color='Scenario'
    )
    + scale_color_manual(values=["#1f77b4", "#ff7f0e"])
    + theme_minimal()
)

p.show()
```
